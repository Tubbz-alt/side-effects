{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scikit-learn.org/stable/_images/grid_search_workflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, matthews_corrcoef, precision_recall_curve, auc\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import SGD\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import EpochScoring\n",
    "from skorch.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import configurations (file paths, etc.)\n",
    "import yaml\n",
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "    \n",
    "configFile = '../cluster/data/medinfmk/ddi/config/config.yml'\n",
    "\n",
    "with open(configFile, 'r') as ymlfile:\n",
    "    cfg = yaml.load(ymlfile, Loader=Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathInput = cfg['filePaths']['dirRaw']\n",
    "pathOutput = cfg['filePaths']['dirProcessed']\n",
    "# path to store python binary files (pickles)\n",
    "# in order not to recalculate them every time\n",
    "pathPickles = cfg['filePaths']['dirProcessedFiles']['dirPickles']\n",
    "pathRuns = cfg['filePaths']['dirProcessedFiles']['dirRuns']\n",
    "pathPaperScores = cfg['filePaths']['dirRawFiles']['paper-individual-metrics-scores']\n",
    "datasetDirs = cfg['filePaths']['dirRawDatasets']\n",
    "DS1_path = str(datasetDirs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../cluster/data/medinfmk/ddi/raw/DS1/paper_individual_metrics_scores.csv'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathPaperScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paperIndividualScores = pd.read_csv(pathPaperScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Similarity</th>\n",
       "      <th>AUC</th>\n",
       "      <th>AUPR</th>\n",
       "      <th>F-measure</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chemical</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>target</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transporter</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enzyme</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pathway</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>indication</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>side effect</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>offside effect</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Similarity    AUC   AUPR   F-measure   Recall   Precision\n",
       "0        chemical  0.631  0.455       0.527    0.899       0.373\n",
       "1          target  0.787  0.642       0.617    0.721       0.540\n",
       "2     transporter  0.682  0.568       0.519    0.945       0.358\n",
       "3          enzyme  0.734  0.599       0.552    0.579       0.529\n",
       "4         pathway  0.767  0.623       0.587    0.650       0.536\n",
       "5      indication  0.802  0.654       0.632    0.740       0.551\n",
       "6     side effect  0.778  0.601       0.619    0.748       0.528\n",
       "7  offside effect  0.782  0.606       0.617    0.764       0.517"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paperIndividualScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir ../cluster/data/medinfmk/ddi/processed/runs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_data(input_fea, input_lab, seperate=False):\n",
    "#     offside_sim_path = input_fea\n",
    "#     drug_interaction_matrix_path = input_lab\n",
    "#     drug_fea = np.loadtxt(offside_sim_path,dtype=float,delimiter=\",\")\n",
    "#     interaction = np.loadtxt(drug_interaction_matrix_path,dtype=int,delimiter=\",\")\n",
    "#     #print(drug_fea.shape)\n",
    "#     #print(interaction.shape)\n",
    "#     #return\n",
    "#     train = []\n",
    "#     label = []\n",
    "#     tmp_fea=[]\n",
    "#     drug_fea_tmp = []\n",
    "#     for i in range(0, interaction.shape[0]):\n",
    "#         for j in range(0, interaction.shape[1]):\n",
    "#             label.append(interaction[i,j])\n",
    "#             drug_fea_tmp = list(drug_fea[i])\n",
    "#             if seperate:\n",
    "        \n",
    "#                  tmp_fea = (drug_fea_tmp,drug_fea_tmp)\n",
    "\n",
    "#             else:\n",
    "#                  tmp_fea = drug_fea_tmp + drug_fea_tmp\n",
    "#             train.append(tmp_fea)\n",
    "\n",
    "#     return np.array(train), np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(input_fea, input_lab, seperate=False):\n",
    "    offside_sim_path = input_fea\n",
    "    drug_interaction_matrix_path = input_lab\n",
    "    drug_fea = np.loadtxt(offside_sim_path,dtype=float,delimiter=\",\")\n",
    "    interaction = np.loadtxt(drug_interaction_matrix_path,dtype=int,delimiter=\",\")\n",
    "    #print(drug_fea.shape)\n",
    "    #print(interaction.shape)\n",
    "    #return\n",
    "    train = []\n",
    "    label = []\n",
    "    tmp_fea=[]\n",
    "    drug_fea_tmp = []\n",
    "            \n",
    "    for i in range(0, (interaction.shape[0]-1)):\n",
    "        for j in range((i+1), interaction.shape[1]):\n",
    "            #print(i,j)\n",
    "    #return\n",
    "            label.append(interaction[i,j])\n",
    "            drug_fea_tmp_1 = list(drug_fea[i])\n",
    "            drug_fea_tmp_2 = list(drug_fea[j])\n",
    "            if seperate:\n",
    "                 tmp_fea = (drug_fea_tmp_1,drug_fea_tmp_2)\n",
    "            else:\n",
    "                 tmp_fea = drug_fea_tmp_1 + drug_fea_tmp_2\n",
    "            train.append(tmp_fea)\n",
    "\n",
    "    return np.array(train), np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_array_format(data):\n",
    "    formated_matrix1 = []\n",
    "    formated_matrix2 = []\n",
    "    for val in data:\n",
    "        formated_matrix1.append(val[0])\n",
    "        formated_matrix2.append(val[1])\n",
    "    return np.array(formated_matrix1), np.array(formated_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels(labels, encoder=None, categorical=True):\n",
    "    if not encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(labels)\n",
    "        y = encoder.transform(labels).astype(np.int32)\n",
    "    if categorical:\n",
    "        y = np_utils.to_categorical(y)\n",
    "        print(y)\n",
    "    return y, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_names(labels, encoder=None, categorical=True):\n",
    "    if not encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(labels)\n",
    "    if categorical:\n",
    "        labels = np_utils.to_categorical(labels)\n",
    "    return labels, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_prep = np.repeat(np.arange(1,6),5).reshape((-1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_prep = np.random.binomial(1, 0.5, size = 25).reshape((5,5))\n",
    "#y_prep = np.arange(0,25).reshape((5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "###input_fea = pathInput+DS1_path+\"/offsideeffect_Jacarrd_sim.csv\"\n",
    "input_fea = pathInput+DS1_path+\"/indication_Jacarrd_sim.csv\"\n",
    "###input_fea = pathInput+DS1_path+\"/dummy/X_dummy.csv\"\n",
    "###input_fea = pathInput+DS1_path+\"/chem_Jacarrd_sim.csv\"\n",
    "###input_fea = pathOutput+\"/finalsimddd.txt\"\n",
    "input_lab = pathInput+DS1_path+\"/drug_drug_matrix.csv\"\n",
    "###input_lab = pathInput+DS1_path+\"/dummy/y_dummy.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_symmetric(a, rtol=1e-05, atol=1e-08):\n",
    "#     return np.allclose(a, a.T, rtol=rtol, atol=atol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(input_fea, X_prep.astype(int), fmt='%i', delimiter=\",\")\n",
    "# np.savetxt(input_lab, y_prep.astype(int), fmt='%i', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,y = prepare_data(input_fea, input_lab, seperate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_data1, X_data2 = transfer_array_format(X)\n",
    "#X = np.concatenate((X_data1, X_data2), axis = 1)\n",
    "###Y, encoder = preprocess_labels(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataPicklePath = pathPickles+\"/data_X_y_chem_Jaccard.p\"\n",
    "#dataPicklePath = pathPickles+\"/data_X_y_offside_Jaccard.p\"\n",
    "dataPicklePath = pathPickles+\"/data_X_y_indication_Jaccard.p\"\n",
    "#dataPicklePath = pathPickles+\"/data_X_y_SNFmat.p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(dataPicklePath, 'wb') as f:\n",
    "#     pickle.dump([X, y], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataPicklePath, 'rb') as f:\n",
    "    X, y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X, y = make_classification(1500, 1000, n_informative=10, random_state=0)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "model_input_dim = X.shape[1]\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(X, y)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tX = torch.from_numpy(X).type(torch.float32)\n",
    "# ty = torch.from_numpy(y).type(torch.int64)\n",
    "\n",
    "# dataSet = TensorDataset(tX, ty)\n",
    "# dataLoader = DataLoader(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def report_available_cuda_devices():\n",
    "#     n_gpu = torch.cuda.device_count()\n",
    "#     print('number of GPUs available:', n_gpu)\n",
    "#     for i in range(n_gpu):\n",
    "#         print(\"cuda:{}, name:{}\".format(i, torch.cuda.get_device_name(i)))\n",
    "#         device = torch.device('cuda', i)\n",
    "#         get_cuda_device_stats(device)\n",
    "#         print()\n",
    "        \n",
    "# def get_cuda_device_stats(device):\n",
    "#     print('total memory available:', torch.cuda.get_device_properties(device).total_memory/(1024**3), 'GB')\n",
    "#     print('total memory allocated on device:', torch.cuda.memory_allocated(device)/(1024**3), 'GB')\n",
    "#     print('max memory allocated on device:', torch.cuda.max_memory_allocated(device)/(1024**3), 'GB')\n",
    "#     print('total memory cached on device:', torch.cuda.memory_cached(device)/(1024**3), 'GB')\n",
    "#     print('max memory cached  on device:', torch.cuda.max_memory_cached(device)/(1024**3), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDD(nn.Module):\n",
    "    def __init__(self, D_in=model_input_dim, H1=300, H2=400, D_out=2, drop=0.5):\n",
    "        super(NDD, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(D_in, H1) # Fully Connected\n",
    "        self.fc2 = nn.Linear(H1, H2)\n",
    "        self.fc3 = nn.Linear(H2, D_out)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "\n",
    "# Model\n",
    "model_input_dim = X.shape[1]\n",
    "D_in, H1, H2, D_out, drop = model_input_dim, 300, 400, 2, 0.5\n",
    "# Training\n",
    "#batch_size, epochs = 100, 20\n",
    "#print_iter = int(epochs / 10)\n",
    "# SGD\n",
    "#learning_rate, momentum, weight_decay, nesterov = 0.01, 0.9, 1e-6, True\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = NDD(D_in, H1, H2, D_out, drop)\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#   print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#   # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "#   model = nn.DataParallel(model)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# #device = \"cpu\"\n",
    "# model.to(device)\n",
    "\n",
    "writer = SummaryWriter(pathRuns+\"test_40epochs_100batch_optim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auc = EpochScoring(scoring='roc_auc', lower_is_better=False)\n",
    "#callbacks.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks.append(TensorBoard(writer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer=SGD(momentum=0.9, weight_decay=1e-6, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    model,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    max_epochs=20,\n",
    "    optimizer=SGD,\n",
    "    optimizer__lr=0.01,\n",
    "    optimizer__momentum=0.9,    \n",
    "    optimizer__weight_decay=1e-6,    \n",
    "    optimizer__nesterov=True,    \n",
    "    batch_size=200,\n",
    "    callbacks=callbacks,\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = Pipeline([\n",
    "#     ('net', net),\n",
    "# ])\n",
    "\n",
    "# pipe.fit(X, y)\n",
    "# y_proba = pipe.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in dataLoader:\n",
    "#     X,y = data\n",
    "#     X = X.to(device)\n",
    "#     y = y.to(device)\n",
    "#     print(\"Outside: input size\", X.size(), y.size(), X.device, y.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'lr': [0.1],\n",
    "#     'max_epochs': [5],\n",
    "#     'module__H1': [300],\n",
    "#     'module__H2': [200, 100],\n",
    "# }\n",
    "# gs = GridSearchCV(net, params, refit=True, cv=3, scoring='accuracy')\n",
    "\n",
    "# gs.fit(X_train, y_train)\n",
    "# print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:933: FutureWarning: Passing attributes to check_is_fitted is deprecated and will be removed in 0.23. The attributes argument is ignored.\n",
      "  \"argument is ignored.\", FutureWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:937: FutureWarning: Passing all_or_any to check_is_fitted is deprecated and will be removed in 0.23. The any_or_all argument is ignored.\n",
      "  \"argument is ignored.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.5854\u001b[0m       \u001b[32m0.7115\u001b[0m        \u001b[35m0.5740\u001b[0m  1.9246\n",
      "      2        \u001b[36m0.4547\u001b[0m       \u001b[32m0.7164\u001b[0m        0.5792  1.9805\n"
     ]
    }
   ],
   "source": [
    "net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = net.predict(X_test)\n",
    "# y_pred_proba = net.predict_proba(X_test)\n",
    "# y_pred_proba = y_pred_proba[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_proba.shape, y_pred.shape\n",
    "# y_pred_proba[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_recall_curve\n",
    "# from sklearn.metrics import plot_precision_recall_curve\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# disp = plot_precision_recall_curve(net, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = net.predict(X_test)\n",
    "\n",
    "# if y_pred.ndim != 1:\n",
    "#     if y_pred.shape[1] != 2:\n",
    "#         raise ValueError(classification_error)\n",
    "#     else:\n",
    "#         y_pred = y_pred[:, 1]\n",
    "\n",
    "pos_label = net.classes_[1]\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred, pos_label=pos_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_pred), auc(precision, recall), f1_score(y_test, y_pred), recall_score(y_test, y_pred), precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
