{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scikit-learn.org/stable/_images/grid_search_workflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, matthews_corrcoef\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import EpochScoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import configurations (file paths, etc.)\n",
    "import yaml\n",
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "    \n",
    "configFile = '../cluster/data/medinfmk/ddi/config/config.yml'\n",
    "\n",
    "with open(configFile, 'r') as ymlfile:\n",
    "    cfg = yaml.load(ymlfile, Loader=Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathInput = cfg['filePaths']['dirRaw']\n",
    "pathOutput = cfg['filePaths']['dirProcessed']\n",
    "# path to store python binary files (pickles)\n",
    "# in order not to recalculate them every time\n",
    "pathPickles = cfg['filePaths']['dirProcessedFiles']['dirPickles']\n",
    "datasetDirs = cfg['filePaths']['dirRawDatasets']\n",
    "DS1_path = str(datasetDirs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(input_fea, input_lab, seperate=False):\n",
    "    offside_sim_path = input_fea\n",
    "    drug_interaction_matrix_path = input_lab\n",
    "    drug_fea = np.loadtxt(offside_sim_path,dtype=float,delimiter=\",\")\n",
    "    interaction = np.loadtxt(drug_interaction_matrix_path,dtype=int,delimiter=\",\")\n",
    "    train = []\n",
    "    label = []\n",
    "    tmp_fea=[]\n",
    "    drug_fea_tmp = []\n",
    "    for i in range(0, interaction.shape[0]):\n",
    "        for j in range(0, interaction.shape[1]):\n",
    "            label.append(interaction[i,j])\n",
    "            drug_fea_tmp = list(drug_fea[i])\n",
    "            if seperate:\n",
    "        \n",
    "                 tmp_fea = (drug_fea_tmp,drug_fea_tmp)\n",
    "\n",
    "            else:\n",
    "                 tmp_fea = drug_fea_tmp + drug_fea_tmp\n",
    "            train.append(tmp_fea)\n",
    "\n",
    "    return np.array(train), np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_array_format(data):\n",
    "    formated_matrix1 = []\n",
    "    formated_matrix2 = []\n",
    "    for val in data:\n",
    "        formated_matrix1.append(val[0])\n",
    "        formated_matrix2.append(val[1])\n",
    "    return np.array(formated_matrix1), np.array(formated_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels(labels, encoder=None, categorical=True):\n",
    "    if not encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(labels)\n",
    "        y = encoder.transform(labels).astype(np.int32)\n",
    "    if categorical:\n",
    "        y = np_utils.to_categorical(y)\n",
    "        print(y)\n",
    "    return y, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_names(labels, encoder=None, categorical=True):\n",
    "    if not encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(labels)\n",
    "    if categorical:\n",
    "        labels = np_utils.to_categorical(labels)\n",
    "    return labels, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###input_fea = pathInput+DS1_path+\"/offsideeffect_Jacarrd_sim.csv\"\n",
    "# input_fea = pathOutput+\"/finalsimddd.txt\"\n",
    "# input_lab = pathInput+DS1_path+\"/drug_drug_matrix.csv\"\n",
    "# X, y = prepare_data(input_fea, input_lab, seperate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_data1, X_data2 = transfer_array_format(X)\n",
    "# X = np.concatenate((X_data1, X_data2), axis = 1)\n",
    "# model_input_dim = X.shape[1]\n",
    "# ###Y, encoder = preprocess_labels(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dataPicklePath = pathPickles+\"/data_X_y_offside_Jaccard\"\n",
    "# dataPicklePath = pathPickles+\"/data_X_y_SNFmat.p\"\n",
    "\n",
    "# with open(dataPicklePath, 'wb') as f:\n",
    "#     pickle.dump([X, y], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataPicklePath, 'rb') as f:\n",
    "    X, y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X, y = make_classification(1500, 1000, n_informative=10, random_state=0)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# tX = torch.from_numpy(X).type(torch.float32)\n",
    "# ty = torch.from_numpy(y).type(torch.int64)\n",
    "\n",
    "# dataSet = TensorDataset(tX, ty)\n",
    "# dataLoader = DataLoader(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def report_available_cuda_devices():\n",
    "#     n_gpu = torch.cuda.device_count()\n",
    "#     print('number of GPUs available:', n_gpu)\n",
    "#     for i in range(n_gpu):\n",
    "#         print(\"cuda:{}, name:{}\".format(i, torch.cuda.get_device_name(i)))\n",
    "#         device = torch.device('cuda', i)\n",
    "#         get_cuda_device_stats(device)\n",
    "#         print()\n",
    "        \n",
    "# def get_cuda_device_stats(device):\n",
    "#     print('total memory available:', torch.cuda.get_device_properties(device).total_memory/(1024**3), 'GB')\n",
    "#     print('total memory allocated on device:', torch.cuda.memory_allocated(device)/(1024**3), 'GB')\n",
    "#     print('max memory allocated on device:', torch.cuda.max_memory_allocated(device)/(1024**3), 'GB')\n",
    "#     print('total memory cached on device:', torch.cuda.memory_cached(device)/(1024**3), 'GB')\n",
    "#     print('max memory cached  on device:', torch.cuda.max_memory_cached(device)/(1024**3), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDD(nn.Module):\n",
    "    def __init__(self, D_in=model_input_dim, H1=300, H2=400, D_out=2, drop=0.5):\n",
    "        super(NDD, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(D_in, H1) # Fully Connected\n",
    "        self.fc2 = nn.Linear(H1, H2)\n",
    "        self.fc3 = nn.Linear(H2, D_out)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "\n",
    "# Model\n",
    "D_in, H1, H2, D_out, drop = model_input_dim, 300, 400, 2, 0.5\n",
    "# Training\n",
    "#batch_size, epochs = 100, 20\n",
    "#print_iter = int(epochs / 10)\n",
    "# SGD\n",
    "#learning_rate, momentum, weight_decay, nesterov = 0.01, 0.9, 1e-6, True\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = NDD(D_in, H1, H2, D_out, drop)\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#   print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#   # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "#   model = nn.DataParallel(model)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# #device = \"cpu\"\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = EpochScoring(scoring='roc_auc', lower_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    model,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    max_epochs=20,\n",
    "    lr=0.01,\n",
    "    batch_size=200,\n",
    "    callbacks=[auc],\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = Pipeline([\n",
    "#     ('net', net),\n",
    "# ])\n",
    "\n",
    "# pipe.fit(X, y)\n",
    "# y_proba = pipe.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in dataLoader:\n",
    "#     X,y = data\n",
    "#     X = X.to(device)\n",
    "#     y = y.to(device)\n",
    "#     print(\"Outside: input size\", X.size(), y.size(), X.device, y.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'lr': [0.1],\n",
    "#     'max_epochs': [5],\n",
    "#     'module__H1': [300],\n",
    "#     'module__H2': [200, 100],\n",
    "# }\n",
    "# gs = GridSearchCV(net, params, refit=False, cv=3, scoring='accuracy')\n",
    "\n",
    "# gs.fit(X, y)\n",
    "# print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    roc_auc    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ---------  ------------  -----------  ------------  ------\n",
      "      1     \u001b[36m0.4994\u001b[0m        \u001b[32m0.6343\u001b[0m       \u001b[35m0.6762\u001b[0m        \u001b[31m0.6296\u001b[0m  3.2965\n",
      "      2     \u001b[36m0.5266\u001b[0m        \u001b[32m0.6297\u001b[0m       0.6762        \u001b[31m0.6295\u001b[0m  3.3285\n",
      "      3     \u001b[36m0.5536\u001b[0m        \u001b[32m0.6296\u001b[0m       0.6762        \u001b[31m0.6294\u001b[0m  3.3204\n",
      "      4     \u001b[36m0.5805\u001b[0m        \u001b[32m0.6294\u001b[0m       0.6762        \u001b[31m0.6292\u001b[0m  3.2844\n",
      "      5     \u001b[36m0.6048\u001b[0m        \u001b[32m0.6293\u001b[0m       0.6762        \u001b[31m0.6290\u001b[0m  3.5208\n",
      "      6     \u001b[36m0.6281\u001b[0m        \u001b[32m0.6291\u001b[0m       0.6762        \u001b[31m0.6288\u001b[0m  3.3566\n",
      "      7     \u001b[36m0.6495\u001b[0m        \u001b[32m0.6288\u001b[0m       0.6762        \u001b[31m0.6285\u001b[0m  3.0010\n",
      "      8     \u001b[36m0.6687\u001b[0m        \u001b[32m0.6285\u001b[0m       0.6762        \u001b[31m0.6282\u001b[0m  3.7872\n",
      "      9     \u001b[36m0.6851\u001b[0m        \u001b[32m0.6282\u001b[0m       0.6762        \u001b[31m0.6278\u001b[0m  3.3166\n",
      "     10     \u001b[36m0.7001\u001b[0m        \u001b[32m0.6276\u001b[0m       0.6762        \u001b[31m0.6272\u001b[0m  3.3314\n",
      "     11     \u001b[36m0.7126\u001b[0m        \u001b[32m0.6271\u001b[0m       0.6762        \u001b[31m0.6265\u001b[0m  3.3916\n",
      "     12     \u001b[36m0.7234\u001b[0m        \u001b[32m0.6263\u001b[0m       0.6762        \u001b[31m0.6255\u001b[0m  3.3175\n",
      "     13     \u001b[36m0.7326\u001b[0m        \u001b[32m0.6250\u001b[0m       0.6762        \u001b[31m0.6240\u001b[0m  3.8125\n",
      "     14     \u001b[36m0.7405\u001b[0m        \u001b[32m0.6235\u001b[0m       0.6762        \u001b[31m0.6220\u001b[0m  3.4449\n",
      "     15     \u001b[36m0.7473\u001b[0m        \u001b[32m0.6210\u001b[0m       0.6762        \u001b[31m0.6189\u001b[0m  3.4357\n",
      "     16     \u001b[36m0.7527\u001b[0m        \u001b[32m0.6174\u001b[0m       0.6762        \u001b[31m0.6144\u001b[0m  3.2707\n",
      "     17     \u001b[36m0.7576\u001b[0m        \u001b[32m0.6119\u001b[0m       0.6762        \u001b[31m0.6076\u001b[0m  3.6170\n",
      "     18     \u001b[36m0.7625\u001b[0m        \u001b[32m0.6042\u001b[0m       0.6762        \u001b[31m0.5975\u001b[0m  3.3555\n",
      "     19     \u001b[36m0.7675\u001b[0m        \u001b[32m0.5927\u001b[0m       0.6762        \u001b[31m0.5832\u001b[0m  3.7359\n",
      "     20     \u001b[36m0.7733\u001b[0m        \u001b[32m0.5783\u001b[0m       \u001b[35m0.6932\u001b[0m        \u001b[31m0.5655\u001b[0m  3.2281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=NDD(\n",
       "    (fc1): Linear(in_features=1096, out_features=300, bias=True)\n",
       "    (fc2): Linear(in_features=300, out_features=400, bias=True)\n",
       "    (fc3): Linear(in_features=400, out_features=2, bias=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = net.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6948691992114657,\n",
       " 0.13743504781986596,\n",
       " 0.5328282162898789,\n",
       " 0.7886776145203112,\n",
       " 0.07527635703679261,\n",
       " 0.17762906382915553)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred), f1_score(y_test, y_pred), roc_auc_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), matthews_corrcoef(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
