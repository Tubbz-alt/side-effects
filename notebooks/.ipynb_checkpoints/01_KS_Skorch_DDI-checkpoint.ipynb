{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scikit-learn.org/stable/_images/grid_search_workflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, matthews_corrcoef\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import SGD\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import EpochScoring\n",
    "from skorch.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import configurations (file paths, etc.)\n",
    "import yaml\n",
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "    \n",
    "configFile = '../cluster/data/medinfmk/ddi/config/config.yml'\n",
    "\n",
    "with open(configFile, 'r') as ymlfile:\n",
    "    cfg = yaml.load(ymlfile, Loader=Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathInput = cfg['filePaths']['dirRaw']\n",
    "pathOutput = cfg['filePaths']['dirProcessed']\n",
    "# path to store python binary files (pickles)\n",
    "# in order not to recalculate them every time\n",
    "pathPickles = cfg['filePaths']['dirProcessedFiles']['dirPickles']\n",
    "pathRuns = cfg['filePaths']['dirProcessedFiles']['dirRuns']\n",
    "datasetDirs = cfg['filePaths']['dirRawDatasets']\n",
    "DS1_path = str(datasetDirs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir ../cluster/data/medinfmk/ddi/processed/runs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(input_fea, input_lab, seperate=False):\n",
    "    offside_sim_path = input_fea\n",
    "    drug_interaction_matrix_path = input_lab\n",
    "    drug_fea = np.loadtxt(offside_sim_path,dtype=float,delimiter=\",\")\n",
    "    interaction = np.loadtxt(drug_interaction_matrix_path,dtype=int,delimiter=\",\")\n",
    "    train = []\n",
    "    label = []\n",
    "    tmp_fea=[]\n",
    "    drug_fea_tmp = []\n",
    "    for i in range(0, interaction.shape[0]):\n",
    "        for j in range(0, interaction.shape[1]):\n",
    "            label.append(interaction[i,j])\n",
    "            drug_fea_tmp = list(drug_fea[i])\n",
    "            if seperate:\n",
    "        \n",
    "                 tmp_fea = (drug_fea_tmp,drug_fea_tmp)\n",
    "\n",
    "            else:\n",
    "                 tmp_fea = drug_fea_tmp + drug_fea_tmp\n",
    "            train.append(tmp_fea)\n",
    "\n",
    "    return np.array(train), np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_array_format(data):\n",
    "    formated_matrix1 = []\n",
    "    formated_matrix2 = []\n",
    "    for val in data:\n",
    "        formated_matrix1.append(val[0])\n",
    "        formated_matrix2.append(val[1])\n",
    "    return np.array(formated_matrix1), np.array(formated_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels(labels, encoder=None, categorical=True):\n",
    "    if not encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(labels)\n",
    "        y = encoder.transform(labels).astype(np.int32)\n",
    "    if categorical:\n",
    "        y = np_utils.to_categorical(y)\n",
    "        print(y)\n",
    "    return y, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_names(labels, encoder=None, categorical=True):\n",
    "    if not encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(labels)\n",
    "    if categorical:\n",
    "        labels = np_utils.to_categorical(labels)\n",
    "    return labels, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###input_fea = pathInput+DS1_path+\"/offsideeffect_Jacarrd_sim.csv\"\n",
    "# input_fea = pathInput+DS1_path+\"/chem_Jacarrd_sim.csv\"\n",
    "# ###input_fea = pathOutput+\"/finalsimddd.txt\"\n",
    "# input_lab = pathInput+DS1_path+\"/drug_drug_matrix.csv\"\n",
    "# X, y = prepare_data(input_fea, input_lab, seperate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_data1, X_data2 = transfer_array_format(X)\n",
    "# X = np.concatenate((X_data1, X_data2), axis = 1)\n",
    "# model_input_dim = X.shape[1]\n",
    "# ###Y, encoder = preprocess_labels(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataPicklePath = pathPickles+\"/data_X_y_chem_Jaccard.p\"\n",
    "#dataPicklePath = pathPickles+\"/data_X_y_offside_Jaccard.p\"\n",
    "dataPicklePath = pathPickles+\"/data_X_y_SNFmat.p\"\n",
    "\n",
    "# with open(dataPicklePath, 'wb') as f:\n",
    "#     pickle.dump([X, y], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataPicklePath, 'rb') as f:\n",
    "    X, y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X, y = make_classification(1500, 1000, n_informative=10, random_state=0)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# tX = torch.from_numpy(X).type(torch.float32)\n",
    "# ty = torch.from_numpy(y).type(torch.int64)\n",
    "\n",
    "# dataSet = TensorDataset(tX, ty)\n",
    "# dataLoader = DataLoader(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def report_available_cuda_devices():\n",
    "#     n_gpu = torch.cuda.device_count()\n",
    "#     print('number of GPUs available:', n_gpu)\n",
    "#     for i in range(n_gpu):\n",
    "#         print(\"cuda:{}, name:{}\".format(i, torch.cuda.get_device_name(i)))\n",
    "#         device = torch.device('cuda', i)\n",
    "#         get_cuda_device_stats(device)\n",
    "#         print()\n",
    "        \n",
    "# def get_cuda_device_stats(device):\n",
    "#     print('total memory available:', torch.cuda.get_device_properties(device).total_memory/(1024**3), 'GB')\n",
    "#     print('total memory allocated on device:', torch.cuda.memory_allocated(device)/(1024**3), 'GB')\n",
    "#     print('max memory allocated on device:', torch.cuda.max_memory_allocated(device)/(1024**3), 'GB')\n",
    "#     print('total memory cached on device:', torch.cuda.memory_cached(device)/(1024**3), 'GB')\n",
    "#     print('max memory cached  on device:', torch.cuda.max_memory_cached(device)/(1024**3), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDD(nn.Module):\n",
    "    def __init__(self, D_in=model_input_dim, H1=400, H2=300, D_out=2, drop=0.5):\n",
    "        super(NDD, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(D_in, H1) # Fully Connected\n",
    "        self.fc2 = nn.Linear(H1, H2)\n",
    "        self.fc3 = nn.Linear(H2, D_out)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "\n",
    "# Model\n",
    "D_in, H1, H2, D_out, drop = model_input_dim, 400, 300, 2, 0.5\n",
    "# Training\n",
    "#batch_size, epochs = 100, 20\n",
    "#print_iter = int(epochs / 10)\n",
    "# SGD\n",
    "#learning_rate, momentum, weight_decay, nesterov = 0.01, 0.9, 1e-6, True\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = NDD(D_in, H1, H2, D_out, drop)\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#   print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#   # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "#   model = nn.DataParallel(model)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# #device = \"cpu\"\n",
    "# model.to(device)\n",
    "\n",
    "writer = SummaryWriter(pathRuns+\"test_40epochs_100batch_optim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auc = EpochScoring(scoring='roc_auc', lower_is_better=False)\n",
    "#callbacks.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks.append(TensorBoard(writer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer=SGD(momentum=0.9, weight_decay=1e-6, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    model,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    max_epochs=30,\n",
    "    optimizer=SGD,\n",
    "    optimizer__lr=0.01,\n",
    "    optimizer__momentum=0.9,    \n",
    "    optimizer__weight_decay=1e-6,    \n",
    "    optimizer__nesterov=True,    \n",
    "    batch_size=100,\n",
    "    callbacks=callbacks,\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = Pipeline([\n",
    "#     ('net', net),\n",
    "# ])\n",
    "\n",
    "# pipe.fit(X, y)\n",
    "# y_proba = pipe.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in dataLoader:\n",
    "#     X,y = data\n",
    "#     X = X.to(device)\n",
    "#     y = y.to(device)\n",
    "#     print(\"Outside: input size\", X.size(), y.size(), X.device, y.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'lr': [0.1],\n",
    "#     'max_epochs': [5],\n",
    "#     'module__H1': [300],\n",
    "#     'module__H2': [200, 100],\n",
    "# }\n",
    "# gs = GridSearchCV(net, params, refit=True, cv=3, scoring='accuracy')\n",
    "\n",
    "# gs.fit(X_train, y_train)\n",
    "# print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6287\u001b[0m       \u001b[32m0.6762\u001b[0m        \u001b[35m0.6251\u001b[0m  4.7235\n",
      "      2        \u001b[36m0.6232\u001b[0m       0.6762        \u001b[35m0.6184\u001b[0m  5.3343\n",
      "      3        \u001b[36m0.6199\u001b[0m       0.6762        \u001b[35m0.6146\u001b[0m  5.5959\n",
      "      4        \u001b[36m0.6176\u001b[0m       \u001b[32m0.6781\u001b[0m        \u001b[35m0.6125\u001b[0m  5.6206\n",
      "      5        \u001b[36m0.6159\u001b[0m       \u001b[32m0.6781\u001b[0m        \u001b[35m0.6125\u001b[0m  4.6541\n",
      "      6        0.6172       0.6779        \u001b[35m0.6110\u001b[0m  5.2221\n",
      "      7        \u001b[36m0.6155\u001b[0m       0.6779        \u001b[35m0.6081\u001b[0m  5.3740\n",
      "      8        \u001b[36m0.6134\u001b[0m       \u001b[32m0.6812\u001b[0m        \u001b[35m0.6058\u001b[0m  5.4916\n",
      "      9        \u001b[36m0.6133\u001b[0m       0.6812        0.6070  5.6058\n",
      "     10        \u001b[36m0.6119\u001b[0m       0.6801        0.6108  6.0737\n",
      "     11        \u001b[36m0.6111\u001b[0m       0.6788        0.6065  5.5411\n",
      "     12        0.6115       \u001b[32m0.6821\u001b[0m        0.6075  5.4129\n",
      "     13        0.6121       0.6797        0.6078  5.4930\n",
      "     14        0.6135       0.6812        \u001b[35m0.6049\u001b[0m  5.7314\n",
      "     15        0.6122       0.6809        0.6061  4.7871\n",
      "     16        0.6127       0.6801        0.6069  5.3251\n",
      "     17        \u001b[36m0.6108\u001b[0m       \u001b[32m0.6855\u001b[0m        \u001b[35m0.6028\u001b[0m  5.4357\n",
      "     18        \u001b[36m0.6095\u001b[0m       0.6838        0.6051  5.1861\n",
      "     19        \u001b[36m0.6093\u001b[0m       0.6819        0.6070  5.8625\n",
      "     20        0.6112       0.6810        0.6056  5.8631\n",
      "     21        0.6097       0.6854        0.6042  5.6442\n",
      "     22        \u001b[36m0.6084\u001b[0m       0.6827        \u001b[35m0.6024\u001b[0m  5.4950\n",
      "     23        0.6100       0.6798        \u001b[35m0.6021\u001b[0m  5.4749\n",
      "     24        0.6110       0.6798        \u001b[35m0.6017\u001b[0m  5.2841\n",
      "     25        0.6092       0.6780        0.6062  5.6555\n",
      "     26        0.6088       0.6790        0.6059  5.4873\n",
      "     27        0.6104       0.6798        \u001b[35m0.6008\u001b[0m  5.5980\n",
      "     28        0.6106       0.6798        \u001b[35m0.5988\u001b[0m  5.3096\n",
      "     29        0.6100       0.6793        0.5994  5.3158\n",
      "     30        0.6087       0.6780        0.6017  5.0076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=NDD(\n",
       "    (fc1): Linear(in_features=1096, out_features=400, bias=True)\n",
       "    (fc2): Linear(in_features=400, out_features=300, bias=True)\n",
       "    (fc3): Linear(in_features=300, out_features=2, bias=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 909,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = net.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5036738368320979,\n",
       " 0.017287071390712276,\n",
       " 0.7491166077738516,\n",
       " 0.008744431611945224)"
      ]
     },
     "execution_count": 911,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred), f1_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
