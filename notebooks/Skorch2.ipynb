{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scikit-learn.org/stable/_images/grid_search_workflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import EpochScoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import configurations (file paths, etc.)\n",
    "import yaml\n",
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "    \n",
    "configFile = '../cluster/data/medinfmk/ddi/config/config.yml'\n",
    "\n",
    "with open(configFile, 'r') as ymlfile:\n",
    "    cfg = yaml.load(ymlfile, Loader=Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathInput = cfg['filePaths']['dirRaw']\n",
    "pathOutput = cfg['filePaths']['dirProcessed']\n",
    "# path to store python binary files (pickles)\n",
    "# in order not to recalculate them every time\n",
    "pathPickles = cfg['filePaths']['dirProcessedFiles']['dirPickles']\n",
    "datasetDirs = cfg['filePaths']['dirRawDatasets']\n",
    "DS1_path = str(datasetDirs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(input_fea, input_lab, seperate=False):\n",
    "    offside_sim_path = input_fea\n",
    "    drug_interaction_matrix_path = input_lab\n",
    "    drug_fea = np.loadtxt(offside_sim_path,dtype=float,delimiter=\",\")\n",
    "    interaction = np.loadtxt(drug_interaction_matrix_path,dtype=int,delimiter=\",\")\n",
    "    train = []\n",
    "    label = []\n",
    "    tmp_fea=[]\n",
    "    drug_fea_tmp = []\n",
    "    for i in range(0, interaction.shape[0]):\n",
    "        for j in range(0, interaction.shape[1]):\n",
    "            label.append(interaction[i,j])\n",
    "            drug_fea_tmp = list(drug_fea[i])\n",
    "            if seperate:\n",
    "        \n",
    "                 tmp_fea = (drug_fea_tmp,drug_fea_tmp)\n",
    "\n",
    "            else:\n",
    "                 tmp_fea = drug_fea_tmp + drug_fea_tmp\n",
    "            train.append(tmp_fea)\n",
    "\n",
    "    return np.array(train), np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_array_format(data):\n",
    "    formated_matrix1 = []\n",
    "    formated_matrix2 = []\n",
    "    for val in data:\n",
    "        formated_matrix1.append(val[0])\n",
    "        formated_matrix2.append(val[1])\n",
    "    return np.array(formated_matrix1), np.array(formated_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels(labels, encoder=None, categorical=True):\n",
    "    if not encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(labels)\n",
    "        y = encoder.transform(labels).astype(np.int32)\n",
    "    if categorical:\n",
    "        y = np_utils.to_categorical(y)\n",
    "        print(y)\n",
    "    return y, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_names(labels, encoder=None, categorical=True):\n",
    "    if not encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(labels)\n",
    "    if categorical:\n",
    "        labels = np_utils.to_categorical(labels)\n",
    "    return labels, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_fea = pathInput+DS1_path+\"/offsideeffect_Jacarrd_sim.csv\"\n",
    "# ###input_fea = pathOutput+\"/finalsimddd.txt\"\n",
    "# input_lab = pathInput+DS1_path+\"/drug_drug_matrix.csv\"\n",
    "# X, y = prepare_data(input_fea, input_lab, seperate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_data1, X_data2 = transfer_array_format(X)\n",
    "# X = np.concatenate((X_data1, X_data2), axis = 1)\n",
    "# model_input_dim = X.shape[1]\n",
    "# ###Y, encoder = preprocess_labels(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataPicklePath = pathPickles+\"/data_X_y.p\"\n",
    "\n",
    "# with open(dataPicklePath, 'wb') as f:\n",
    "#     pickle.dump([X, y], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataPicklePath, 'rb') as f:\n",
    "    X, y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X, y = make_classification(1500, 1000, n_informative=10, random_state=0)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "# tX = torch.from_numpy(X).type(torch.float32)\n",
    "# ty = torch.from_numpy(y).type(torch.int64)\n",
    "\n",
    "# dataSet = TensorDataset(tX, ty)\n",
    "# dataLoader = DataLoader(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def report_available_cuda_devices():\n",
    "#     n_gpu = torch.cuda.device_count()\n",
    "#     print('number of GPUs available:', n_gpu)\n",
    "#     for i in range(n_gpu):\n",
    "#         print(\"cuda:{}, name:{}\".format(i, torch.cuda.get_device_name(i)))\n",
    "#         device = torch.device('cuda', i)\n",
    "#         get_cuda_device_stats(device)\n",
    "#         print()\n",
    "        \n",
    "# def get_cuda_device_stats(device):\n",
    "#     print('total memory available:', torch.cuda.get_device_properties(device).total_memory/(1024**3), 'GB')\n",
    "#     print('total memory allocated on device:', torch.cuda.memory_allocated(device)/(1024**3), 'GB')\n",
    "#     print('max memory allocated on device:', torch.cuda.max_memory_allocated(device)/(1024**3), 'GB')\n",
    "#     print('total memory cached on device:', torch.cuda.memory_cached(device)/(1024**3), 'GB')\n",
    "#     print('max memory cached  on device:', torch.cuda.max_memory_cached(device)/(1024**3), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDD(nn.Module):\n",
    "    def __init__(self, D_in=model_input_dim, H1=500, H2=300, D_out=2, drop=0.5):\n",
    "        super(NDD, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(D_in, H1) # Fully Connected\n",
    "        self.fc2 = nn.Linear(H1, H2)\n",
    "        self.fc3 = nn.Linear(H2, D_out)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "\n",
    "# Model\n",
    "D_in, H1, H2, D_out, drop = model_input_dim, 400, 300, 2, 0.5\n",
    "# Training\n",
    "#batch_size, epochs = 100, 20\n",
    "#print_iter = int(epochs / 10)\n",
    "# SGD\n",
    "#learning_rate, momentum, weight_decay, nesterov = 0.01, 0.9, 1e-6, True\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = NDD(D_in, H1, H2, D_out, drop)\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#   print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#   # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "#   model = nn.DataParallel(model)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# #device = \"cpu\"\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = EpochScoring(scoring='roc_auc', lower_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    model,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    #max_epochs=10,\n",
    "    #lr=0.1,\n",
    "    callbacks=[auc],\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = Pipeline([\n",
    "#     ('net', net),\n",
    "# ])\n",
    "\n",
    "# pipe.fit(X, y)\n",
    "# y_proba = pipe.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in dataLoader:\n",
    "#     X,y = data\n",
    "#     X = X.to(device)\n",
    "#     y = y.to(device)\n",
    "#     print(\"Outside: input size\", X.size(), y.size(), X.device, y.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module because the following parameters were re-set: H1, H2.\n",
      "Re-initializing module because the following parameters were re-set: H1, H2.\n",
      "  epoch    roc_auc    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ---------  ------------  -----------  ------------  ------\n",
      "      1     \u001b[36m0.6455\u001b[0m        \u001b[32m0.5812\u001b[0m       \u001b[35m0.6764\u001b[0m        \u001b[31m0.6050\u001b[0m  3.5251\n",
      "      2     \u001b[36m0.6872\u001b[0m        \u001b[32m0.5529\u001b[0m       0.6764        0.7468  3.5309\n",
      "      3     0.4552        \u001b[32m0.5422\u001b[0m       0.6764        0.9118  3.1823\n",
      "      4     0.5310        \u001b[32m0.5350\u001b[0m       0.6764        0.7842  3.8062\n",
      "      5     0.4786        \u001b[32m0.5296\u001b[0m       0.6764        0.9074  3.6634\n",
      "Re-initializing module because the following parameters were re-set: H1, H2.\n",
      "Re-initializing module because the following parameters were re-set: H1, H2.\n",
      "  epoch    roc_auc    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ---------  ------------  -----------  ------------  ------\n",
      "      1     \u001b[36m0.6350\u001b[0m        \u001b[32m0.5739\u001b[0m       \u001b[35m0.6764\u001b[0m        \u001b[31m0.6160\u001b[0m  3.6142\n",
      "      2     \u001b[36m0.6436\u001b[0m        \u001b[32m0.5335\u001b[0m       0.6764        \u001b[31m0.6123\u001b[0m  3.6264\n",
      "      3     0.6402        \u001b[32m0.5165\u001b[0m       0.6146        0.6341  3.5743\n",
      "      4     0.6388        \u001b[32m0.5048\u001b[0m       0.6626        0.6211  3.4833\n",
      "      5     0.6388        \u001b[32m0.4970\u001b[0m       0.5135        0.7042  3.2542\n",
      "Re-initializing module because the following parameters were re-set: H1, H2.\n",
      "Re-initializing module because the following parameters were re-set: H1, H2.\n",
      "  epoch    roc_auc    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ---------  ------------  -----------  ------------  ------\n",
      "      1     \u001b[36m0.6489\u001b[0m        \u001b[32m0.5794\u001b[0m       \u001b[35m0.6764\u001b[0m        \u001b[31m0.6156\u001b[0m  3.5759\n",
      "      2     0.6473        \u001b[32m0.5533\u001b[0m       0.6764        \u001b[31m0.6004\u001b[0m  3.5830\n",
      "      3     \u001b[36m0.6604\u001b[0m        \u001b[32m0.5365\u001b[0m       0.6764        0.6058  3.6285\n",
      "      4     \u001b[36m0.6622\u001b[0m        \u001b[32m0.5259\u001b[0m       \u001b[35m0.6815\u001b[0m        0.6057  3.6593\n",
      "      5     0.6274        \u001b[32m0.5175\u001b[0m       0.4004        0.8924  3.6708\n",
      "Re-initializing module because the following parameters were re-set: H1, H2.\n",
      "Re-initializing module because the following parameters were re-set: H1, H2.\n",
      "  epoch    roc_auc    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ---------  ------------  -----------  ------------  ------\n",
      "      1     \u001b[36m0.7090\u001b[0m        \u001b[32m0.5839\u001b[0m       \u001b[35m0.6764\u001b[0m        \u001b[31m0.6035\u001b[0m  3.6186\n",
      "      2     0.7080        \u001b[32m0.5559\u001b[0m       0.6764        0.7177  3.5847\n",
      "      3     0.4753        \u001b[32m0.5448\u001b[0m       0.6764        0.6929  3.5826\n",
      "      4     0.4751        \u001b[32m0.5376\u001b[0m       0.6764        0.8759  3.8585\n",
      "      5     0.5362        \u001b[32m0.5314\u001b[0m       0.6764        0.7393  3.7508\n",
      "Re-initializing module because the following parameters were re-set: H1, H2.\n",
      "Re-initializing module because the following parameters were re-set: H1, H2.\n",
      "  epoch    roc_auc    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ---------  ------------  -----------  ------------  ------\n",
      "      1     \u001b[36m0.5954\u001b[0m        \u001b[32m0.5759\u001b[0m       \u001b[35m0.5649\u001b[0m        \u001b[31m0.6755\u001b[0m  3.7970\n",
      "      2     \u001b[36m0.6062\u001b[0m        \u001b[32m0.5371\u001b[0m       \u001b[35m0.6027\u001b[0m        \u001b[31m0.6730\u001b[0m  3.7480\n",
      "      3     \u001b[36m0.6347\u001b[0m        \u001b[32m0.5202\u001b[0m       \u001b[35m0.6764\u001b[0m        \u001b[31m0.5995\u001b[0m  3.6666\n",
      "      4     \u001b[36m0.6407\u001b[0m        \u001b[32m0.5073\u001b[0m       0.6209        0.6265  3.9557\n",
      "      5     0.6302        \u001b[32m0.4999\u001b[0m       0.6764        0.6255  3.5859\n",
      "Re-initializing module because the following parameters were re-set: H1, H2.\n",
      "Re-initializing module because the following parameters were re-set: H1, H2.\n",
      "  epoch    roc_auc    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ---------  ------------  -----------  ------------  ------\n",
      "      1     \u001b[36m0.6426\u001b[0m        \u001b[32m0.5804\u001b[0m       \u001b[35m0.6764\u001b[0m        \u001b[31m0.6196\u001b[0m  3.6174\n",
      "      2     \u001b[36m0.6543\u001b[0m        \u001b[32m0.5536\u001b[0m       0.6764        \u001b[31m0.6138\u001b[0m  3.2918\n",
      "      3     \u001b[36m0.6544\u001b[0m        \u001b[32m0.5368\u001b[0m       0.6604        0.6289  3.5282\n",
      "      4     \u001b[36m0.6595\u001b[0m        \u001b[32m0.5264\u001b[0m       \u001b[35m0.6847\u001b[0m        \u001b[31m0.5960\u001b[0m  3.7312\n",
      "      5     \u001b[36m0.6605\u001b[0m        \u001b[32m0.5176\u001b[0m       \u001b[35m0.6879\u001b[0m        0.6066  3.6865\n",
      "0.5820568490596196 {'lr': 0.1, 'max_epochs': 5, 'module__H1': 300, 'module__H2': 100}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'lr': [0.1],\n",
    "    'max_epochs': [5],\n",
    "    'module__H1': [300],\n",
    "    'module__H2': [200, 100],\n",
    "}\n",
    "gs = GridSearchCV(net, params, refit=False, cv=3, scoring='accuracy')\n",
    "\n",
    "gs.fit(X, y)\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
